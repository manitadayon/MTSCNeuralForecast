{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright  (c) 2021  California  Institute  of Technology (\"Caltech\"). U.S. Government sponsorship acknowledged.\n",
    "\n",
    "All  rights  reserved.\n",
    " \n",
    "Redistribution  and  use  in  source  and  binary  forms,  with  or  without  modification,  are  permitted  provided that the  following  conditions are  met:\n",
    " \n",
    "- Redistributions  of  source  code  must  retain  the  above  copyright  notice,  this  list  of  conditions  and the  following  disclaimer.\n",
    "- Redistributions  in  binary  form  must  reproduce  the  above  copyright  notice,  this  list  of  conditions and  the  following  disclaimer  in  the  documentation  and/or other materials provided  with  the distribution.\n",
    "- Neither  the  name  of  Caltech  nor  its  operating  division,  the  Jet  Propulsion  Laboratory,  nor  the names  of  its  contributors  may  be  used  to  endorse  or  promote  products  derived  from  this  software without  specific  prior  written  permission.\n",
    "\n",
    "THIS  SOFTWARE  IS  PROVIDED  BY  THE  COPYRIGHT  HOLDERS  AND  CONTRIBUTORS  \"AS IS\" AND  ANY  EXPRESS  OR  IMPLIED  WARRANTIES, INCLUDING, BUT  NOT  LIMITED  TO, THE  IMPLIED  WARRANTIES  OF  MERCHANTABILITY  AND  FITNESS  FOR A  PARTICULAR PURPOSE ARE DISCLAIMED. \n",
    "IN NO EVENT SHALL THE  COPYRIGHT  OWNER OR CONTRIBUTORS BE  LIABLE  FOR  ANY  DIRECT,  INDIRECT,  INCIDENTAL,  SPECIAL, EXEMPLARY, OR  CONSEQUENTIAL  DAMAGES (INCLUDING,  BUT  NOT  LIMITED  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS  OF  USE, DATA, OR  PROFITS; OR  BUSINESS  INTERRUPTION)  HOWEVER  CAUSED  AND  ON  ANY  THEORY  OF  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)  ARISING  IN  ANY  WAY  OUT  OF  THE  USE  OF  THIS  SOFTWARE,  EVEN  IF ADVISED OF  THE  POSSIBILITY  OF  SUCH  DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# This is written to combine clustering and forecasting \n",
    "\n",
    "# Data can be clustered using feature and distance based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from numpy import array\n",
    "import os\n",
    "from pandas import concat\n",
    "from numpy import concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard\n",
    "from keras import optimizers\n",
    "import random \n",
    "import math\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Input, Dense,Flatten,concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import xlrd\n",
    "from keras.models import model_from_json\n",
    "from joblib import dump, load\n",
    "import shutil\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the static data.\n",
    "Static_data=pd.read_csv('/Desktop/Static_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_series_Path= sorted(glob.glob('/Desktop/Dynamic_Data/*.csv'))\n",
    "Time_series_Path.sort(key=os.path.getmtime)\n",
    "Time_series_Path=[ii.split('/')[-1].split('.')[0] for ii in Time_series_Path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model Weights and Architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Model_Weight(model,Weight_Loc,Layout_Loc):\n",
    "    # Save the weights\n",
    "    model.save_weights(Weight_Loc)\n",
    "\n",
    "    # Save the model architecture\n",
    "    with open(Layout_Loc, 'w') as f:\n",
    "        f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Prediction to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Prediction_To_File(Value,Header,File_Loc):\n",
    "    with open(File_Loc, 'w') as filehandle:\n",
    "        filehandle.write(Header)\n",
    "        filehandle.write('\\n')\n",
    "        for listitem in Value:\n",
    "            filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Sklearn Models like MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sklean_Save(Model,flag,Model_Loc): # Flag determines to load or save the model. Joblib did not work\n",
    "    if(flag==0):   # Saving\n",
    "        dump(Model, Model_Loc)              \n",
    "    elif(flag==1):\n",
    "        Model = load(Model_Loc)    # This was for .joblib\n",
    "        return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Permute_wells(File_Name,Train_Ratio): \n",
    "    A=np.arange(1,len(File_Name)+1)\n",
    "    random.shuffle(A) \n",
    "    Train_Wells_ID=A[0:math.floor(Train_Ratio*len(File_Name))]\n",
    "    Test_Wells_ID=[ii for ii in A if ii not in Train_Wells_ID]\n",
    "    return [Train_Wells_ID,Test_Wells_ID]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Permute_wells_static_dynamic(Time_Well,Static_Data,Train_Ratio): \n",
    "    A=np.arange(0,len(Time_Well))\n",
    "    random.shuffle(A) \n",
    "    Train_Wells_ID=A[0:math.floor(Train_Ratio*len(Time_Well))]\n",
    "    Test_Wells_ID=[ii for ii in A if ii not in Train_Wells_ID]\n",
    "    Train_Wells_ID.sort()\n",
    "    Test_Wells_ID.sort()\n",
    "    Train_Static=Static_Data.values[Train_Wells_ID,:]  \n",
    "    Test_Static= Static_Data.values[array(Test_Wells_ID),:]\n",
    "    return [Train_Wells_ID,Test_Wells_ID,Train_Static,Test_Static]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "File1= Time_series_Path\n",
    "File2=Static_data\n",
    "Train_Ratio=0.9\n",
    "Static_Time_Well=Permute_wells_static_dynamic(File1,File2,Train_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Across the well , Training for first N days and predict the Kth days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the function to choose training on first N days for feat and predict the Kth days.\n",
    "## File_Name is the name of the wells.\n",
    "## Train_ID and Test_ID are the ID or the number associated with the wells.\n",
    "## Feat is the column of the measurement for the data after removing the time.\n",
    "## N is the number of days used for Training.\n",
    "## K is the number used for kth days testing. \n",
    "def Data_N_days_Train_predict_K_days(File_Name,subpath,Train_ID,Test_ID,Feat,N,K):\n",
    "    Temp_Train=[]\n",
    "    Target_Train=[]\n",
    "    Temp_Test=[]\n",
    "    Target_Test=[]\n",
    "    Gas_column=2\n",
    "    for count,ii in enumerate(File_Name):   \n",
    "        for jj in Train_ID:\n",
    "            if(count== jj):\n",
    "                Path_new=subpath+ii+'.csv'\n",
    "                Data_file= pd.read_csv(Path_new)\n",
    "                Data_file.drop('Time', axis=1, inplace=True)\n",
    "                Data=Data_file.iloc[0:N,np.r_[Feat]].values  \n",
    "                Temp_Train.append(Data)\n",
    "                Target_Train.append(Data_file.iloc[K-1,Gas_column]) # index starts from 0 and this is the Kth days.\n",
    "                 \n",
    "    Train_Data=np.concatenate(Temp_Train)\n",
    "    for count1,mm in enumerate(File_Name):  \n",
    "\n",
    "        for kk in Test_ID:\n",
    "            if(count1==kk):\n",
    "                Path_new=subpath+mm+'.csv'\n",
    "                Data_file= pd.read_csv(Path_new)\n",
    "                Data_file.drop('Time', axis=1, inplace=True)\n",
    "                Data=Data_file.iloc[0:N,np.r_[Feat]].values  \n",
    "                Temp_Test.append(Data)\n",
    "                Target_Test.append(Data_file.iloc[K-1,Gas_column])\n",
    "    Test_Data=np.concatenate(Temp_Test)\n",
    "    return [Train_Data,np.array(Target_Train),Test_Data,np.array(Target_Test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data for LSTM to 3D Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Reshaping(Data,num_step,feat):\n",
    "    return Data.reshape(-1,num_step,feat)   # Reshape to 3D Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Different Variations of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perfrom Training Using LSTM, Stacked LSTM and Bidirectional LSTM, etc\n",
    "def LSTM_Training(X,X_static,y,X_test,X_Static_test,y_test,num_step,feat,static_dim,config,flag,TensorBoard_Path,Verbose):\n",
    "    batch_size,n_epochs,num_node,num_out=config\n",
    "    dir_path=TensorBoard_Path\n",
    "    callbacks = [EarlyStopping(monitor='loss', patience=2)]\n",
    "    if(flag==0):    # Vanila LSTM\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_node, activation='tanh', input_shape=(num_step, feat)))\n",
    "        model.add(Dense(num_out))\n",
    "        model.compile(optimizer='adagrad', loss='mse')\n",
    "        history=model.fit(X,y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=(X_test,y_test),callbacks=callbacks)\n",
    "    elif(flag==1):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_node, activation='tanh', return_sequences=True, input_shape=(num_step, feat)))\n",
    "        model.add(LSTM(num_node, activation='tanh'))\n",
    "        model.add(Dense(num_out))\n",
    "        model.compile(optimizer='adagrad', loss='mse')\n",
    "        history=model.fit(X,y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=(X_test,y_test),callbacks=callbacks)\n",
    "    elif(flag==2):\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(num_node, activation='tanh'), input_shape=(num_step, feat)))\n",
    "        model.add(Dense(num_out))\n",
    "        model.compile(optimizer='adagrad', loss='mse')\n",
    "        history=model.fit(X,y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=(X_test,y_test),callbacks=callbacks)\n",
    "    elif(flag==3):  # Functional API to add static features and a vanilia LSTM\n",
    "        Input1=Input(shape=(num_step, feat))\n",
    "        LSTM_1_1 = LSTM(num_node, activation='tanh',return_sequences=True)(Input1)\n",
    "        LSTM_1_1 = LSTM(num_node, activation='tanh',return_sequences=False)(LSTM_1_1)\n",
    "        Dense_1_2 = Dense(num_node)(LSTM_1_1)\n",
    "        Input2=Input(shape=(static_dim,))\n",
    "        Dense_2_1 = Dense(num_node)(Input2)\n",
    "        merge = concatenate([Dense_1_2, Dense_2_1])\n",
    "        Final_out=Dense(num_out)(merge)\n",
    "        ### Compilation\n",
    "        model = Model(inputs=[Input1,Input2], outputs=Final_out)\n",
    "        model.compile(optimizer='adagrad',loss='mse')\n",
    "        history=model.fit([X,X_static],y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=([X_test,X_Static_test],y_test),callbacks=callbacks)\n",
    "    elif (flag==4):\n",
    "        Input1=Input(shape=(num_step, feat))\n",
    "        LSTM_1_1 = LSTM(num_node, activation='tanh',return_sequences=False)(Input1)\n",
    "        Dense_1_2 = Dense(num_node)(LSTM_1_1)\n",
    "        Input2=Input(shape=(static_dim,))\n",
    "        Dense_2_1 = Dense(num_node)(Input2)\n",
    "        merge = concatenate([Dense_1_2, Dense_2_1])\n",
    "\n",
    "        Final_out=Dense(num_out)(merge)\n",
    "        ### Compilation\n",
    "        model = Model(inputs=[Input1,Input2], outputs=Final_out)\n",
    "        model.compile(optimizer='adagrad',loss='mse')\n",
    "        history=model.fit([X,X_static],y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=([X_test,X_Static_test],y_test),callbacks=callbacks)\n",
    "    elif (flag==5):\n",
    "        Input1=Input(shape=(num_step, feat))\n",
    "        LSTM_1_1 = LSTM(num_node, activation='tanh',return_sequences=False)(Input1)\n",
    "        Input2=Input(shape=(static_dim,))\n",
    "        merge = concatenate([LSTM_1_1, Input2])\n",
    "\n",
    "        Dense_3=Dense(num_node,activation='tanh')(merge)\n",
    "        Final_out=Dense(num_out)(Dense_3)\n",
    "        ### Compilation\n",
    "        model = Model(inputs=[Input1,Input2], outputs=Final_out)\n",
    "        model.compile(optimizer='adagrad',loss='mse')\n",
    "        history=model.fit([X,X_static],y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=([X_test,X_Static_test],y_test),callbacks=callbacks)\n",
    "    elif (flag==6):\n",
    "        Input1=Input(shape=(num_step, feat))\n",
    "        LSTM_1_1 = LSTM(num_node, activation='tanh',return_sequences=True)(Input1)\n",
    "        Input2=Input(shape=(static_dim,))\n",
    "        flat_2= Flatten()(LSTM_1_1)\n",
    "        merge = concatenate([flat_2, Input2])\n",
    "\n",
    "        Dense_3=Dense(num_node,activation='tanh')(merge)\n",
    "        Final_out=Dense(num_out)(Dense_3)\n",
    "        ### Compilation\n",
    "        model = Model(inputs=[Input1,Input2], outputs=Final_out)\n",
    "        model.compile(optimizer='adagrad',loss='mse')\n",
    "        history=model.fit([X,X_static],y,epochs=n_epochs, batch_size=batch_size,shuffle=False, verbose=Verbose,validation_data=([X_test,X_Static_test],y_test),callbacks=callbacks)\n",
    "        \n",
    "    return [model,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train Loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Validation Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Validation Loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Zeros Values from the Ground Truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zero_Removal(true,pred,thresh,flag):  \n",
    "    if(flag==0):   # Remove where true value is zero.\n",
    "        x=(np.where(true==0))   # Get the row and column where true value is zero.\n",
    "        New_true=np.delete(true,x[0])  # Remove the rows, since it is 1D array\n",
    "        New_pred=np.delete(pred,x[0])\n",
    "    elif(flag==1): # This is better remove rows where MAPE value is higher than a threshold\n",
    "        true=array(true).flatten()\n",
    "        pred=array(pred).flatten()\n",
    "        x=np.abs((true -pred) / true).tolist()\n",
    "        ii=[ii for ii,v in enumerate(x) if v > thresh]\n",
    "        New_true=np.delete(true,ii)  # Remove the rows, since it is 1D array\n",
    "        New_pred=np.delete(pred,ii)\n",
    "            \n",
    "    return [New_true,New_pred,ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAPE Error (Mean Absolute Percantage Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_prediction(X,X_static,Model,num_step,feat,scaler,Method,static_dim,Flag):\n",
    "    Pred=[]\n",
    "    if(Method=='Dynamic'):\n",
    "        for ii in range(X.shape[0]):\n",
    "            Prediction=Model.predict(X[ii,:,:].reshape(1,num_step,feat), verbose=0)\n",
    "            Prediction2=scaler.inverse_transform(Prediction)\n",
    "            Pred.append(Prediction2[0][0])\n",
    "    elif (Method == 'Dynamic_Static' and Flag=='All_Static'):  # Use all static feature for one prediction\n",
    "        for ii in range(X.shape[0]):\n",
    "            Prediction=Model.predict([X[ii,:,:].reshape(1,num_step,feat),X_static], verbose=0) \n",
    "            Prediction2=scaler.inverse_transform(Prediction)\n",
    "            Pred.append(Prediction2[0][0])\n",
    "    elif (Method == 'Dynamic_Static' and Flag=='Static'):    # Use only corresponding static feature for dynamic feature.\n",
    "        for ii in range(X.shape[0]):\n",
    "            Prediction=Model.predict([X[ii,:,:].reshape(1,num_step,feat),X_static[ii,:].reshape(1,static_dim)], verbose=0)\n",
    "            Prediction2=scaler.inverse_transform(Prediction)\n",
    "            Pred.append(Prediction2[0][0])\n",
    "        \n",
    "    return Pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing data based on cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clustered_Data(File,str1,substr1):   # File is a csv file containing well name and cluster labels\n",
    "    All_cluster=[]\n",
    "    Data=pd.read_csv(File)\n",
    "    N_cluster=len(np.unique(Data.iloc[:,1]))\n",
    "    for jj in range(1,N_cluster+1):\n",
    "        clustered_Data=[]\n",
    "        for count,ii in enumerate(Data.iloc[:,1]):\n",
    "            if(ii==jj):\n",
    "                clustered_Data.append(str(Data.iloc[count,0]).replace(str1,substr1))\n",
    "        All_cluster.append(clustered_Data)\n",
    "    return All_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "File='Loc of cluster labels'\n",
    "Clustered_Data=Clustered_Data(File,'.csv','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clustered_Data2=[]   # Convert numpy.int64 to str\n",
    "for ii in range(len(Clustered_Data)):\n",
    "    Temp=[]\n",
    "    for jj in Clustered_Data[ii]:\n",
    "        Temp.append(str(jj))\n",
    "    Clustered_Data2.append(Temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Cross_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  it does the K-fold CV and is independent of \n",
    "## Train and Test Split and avoid accuracy or loss by chance. \n",
    "## Howver this model can not be saved since we do not know which run to save. \n",
    "## One way is to save the run that result in the lowest error and save the train and test ID split that result in the \n",
    "## minimum error.\n",
    "def K_Fold_CV_LSTM_Forecasting(File_Name1,File_Name2,subpath,Train_Ratio,K,Training_Day,Test_day,col_num,Min,Max,num_step,feat,static_dim,config, TensorBoard_Path,train_flag,thresh,outlier_flag,Verbose,Static_Flag):\n",
    "    SUM_RMSE=0       \n",
    "    SUM_MAPE=0        \n",
    "    SUM_MAE=0\n",
    "    All_outlier=[]   \n",
    "    Temp=[]         \n",
    "    MAPE_Hold=math.inf      \n",
    "    Best_Model=0\n",
    "    All_Test_ID=[]\n",
    "    Best_pred=0         \n",
    "    All_outlier_ID=[]   \n",
    "    for ii in range(K):\n",
    "        Wells_Number=Permute_wells_static_dynamic(File_Name1,File_Name2,Train_Ratio)\n",
    "        Train_ID,Test_ID=Wells_Number[0],Wells_Number[1]\n",
    "        Train_ID=Train_ID.tolist()\n",
    "        Static_Train,Static_Test=Wells_Number[2],Wells_Number[3]\n",
    "     \n",
    "        DATA=Data_N_days_Train_predict_K_days(File_Name1,subpath,Train_ID,Test_ID,col_num,Training_Day,Test_day)\n",
    "        X_scaler = MinMaxScaler(feature_range=(Min,Max))\n",
    "        y_scaler = MinMaxScaler(feature_range=(Min,Max))\n",
    "        X_train= X_scaler.fit_transform(DATA[0])\n",
    "        y_train= y_scaler.fit_transform(DATA[1].reshape(-1,1))\n",
    "        X_test= X_scaler.transform(DATA[2])\n",
    "        y_test= y_scaler.transform(DATA[3].reshape(-1,1))\n",
    "        \n",
    "        Static_scaler = MinMaxScaler(feature_range=(Min,Max))\n",
    "        Static_Train=Static_scaler.fit_transform(Static_Train)\n",
    "        Static_Test=Static_scaler.transform(Static_Test)\n",
    "        \n",
    "        X_train=LSTM_Reshaping(X_train,num_step,feat)\n",
    "        X_test=LSTM_Reshaping(X_test,num_step,feat)\n",
    "        y_true=DATA[3].reshape(-1,1)\n",
    "      \n",
    "        LSTM_model,history=LSTM_Training(X_train,Static_Train,y_train,X_test,Static_Test,y_test,num_step,feat,static_dim,config,train_flag,TensorBoard_Path,Verbose)\n",
    "        Pred=LSTM_prediction(X_test,Static_Test,LSTM_model,num_step,feat,y_scaler,'Dynamic',static_dim,Static_Flag)\n",
    "        New_True,New_Pred,outlier=Zero_Removal(y_true,Pred,thresh,outlier_flag)\n",
    "        All_outlier_ID.append(outlier)\n",
    "        All_outlier=list(set(outlier+Temp))\n",
    "        Temp=All_outlier    \n",
    "        RMSE = measure_rmse(New_True,np.array(New_Pred))\n",
    "        MAPE = mean_absolute_percentage_error(New_True,np.array(New_Pred))\n",
    "        MAE=mean_absolute_error(New_True, np.array(New_Pred))\n",
    "        if (MAPE <= MAPE_Hold ):\n",
    "            Best_Model= LSTM_model\n",
    "            MAPE_Hold= MAPE\n",
    "            Best_y_scaler=y_scaler\n",
    "            Best_X_scaler=X_scaler\n",
    "            Best_Static_scaler=Static_scaler\n",
    "            Best_pred=Pred\n",
    "\n",
    "        RMSE_TOT=SUM_RMSE+RMSE\n",
    "        MAPE_TOT=SUM_MAPE+MAPE\n",
    "        MAE_TOT=SUM_MAE+MAE\n",
    "\n",
    "        SUM_RMSE=RMSE_TOT\n",
    "        SUM_MAPE=MAPE_TOT\n",
    "        SUM_MAE=MAE_TOT\n",
    "        \n",
    "        All_Test_ID.append(Test_ID)\n",
    "        print(ii)\n",
    "    RMSE_AVG=SUM_RMSE/K\n",
    "    MAPE_AVG=SUM_MAPE/K\n",
    "    MAE_AVG=SUM_MAE/K\n",
    "    return [RMSE_AVG,MAPE_AVG,MAE_AVG,All_outlier_ID,Best_Model,All_Test_ID,Best_X_scaler,Best_y_scaler,Best_Static_scaler]   # We also want to save the model with the lowest MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Testing and saving parameters using CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Static_data_Important_Feature=Static_data.iloc[:,np.r_[1:5]]   # Extracting the useful static data, the column number might be different for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Static Data Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Static_data_Important_Feature.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to data in common between time series and Static Data\n",
    "Path='/Users/Downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "File1= Clustered_Data2[0]   # Cluster 1, do this one for all the clusters and try to optimize the model per cluster\n",
    "Start=time.time()\n",
    "# File1= Time_series_Path   # With No Clustering\n",
    "File2=Static_data_Important_Feature\n",
    "Train_Ratio=0.9\n",
    "subpath=Path\n",
    "Training_Day=100\n",
    "Test_day=400\n",
    "col_num=[0,1,2]\n",
    "Min=-1\n",
    "Max=1\n",
    "num_step=Training_Day\n",
    "feat=3\n",
    "static_dim=File2.shape[1]\n",
    "config=[32,80,30,1]\n",
    "Tensorboard_Path='/Users/Downloads/'\n",
    "thresh=10    \n",
    "outlier_flag=1\n",
    "K=5\n",
    "Verbose=0\n",
    "Static_Flag='Static'    # Static means use the corresponding static data for the dynamic feature\n",
    "train_flag=0 \n",
    "N_run=1\n",
    "outlier_ALL=[]\n",
    "SUM_MAPE=0\n",
    "SUM_RMSE=0\n",
    "SUM_MAE=0\n",
    "for ii in range(N_run):\n",
    "    RMSE,MAPE,MAE,Outlier_CV,Model_Trained,All_Test_ID,X_scaler,y_scaler,Static_scaler= K_Fold_CV_LSTM_Forecasting(File1,File2,subpath,Train_Ratio,K,Training_Day,Test_day,col_num,Min,Max,num_step,feat,static_dim,config, Tensorboard_Path,train_flag,thresh,outlier_flag,Verbose,Static_Flag)\n",
    "    outlier_ALL.append(Outlier_CV)  # To keep track of all outlier\n",
    "    SUM_MAPE=SUM_MAPE+MAPE\n",
    "    SUM_RMSE=SUM_RMSE+RMSE\n",
    "    SUM_MAE=SUM_MAE+MAE\n",
    "MAPE_AVG=SUM_MAPE/N_run\n",
    "RMSE_AVG=SUM_RMSE/N_run\n",
    "MAE_AVG=SUM_MAE/N_run\n",
    "END=time.time()\n",
    "print(\"Time Takes is\",END-Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(MAPE,Outlier_CV,All_Test_ID,sep='\\n')\n",
    "print(RMSE_AVG,MAPE_AVG,MAE_AVG,outlier_ALL,All_Test_ID,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(Model_Trained, to_file='/Users/Downloads/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save MinMaxScaler Model for both X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_x=X_scaler\n",
    "Model_y=y_scaler\n",
    "Model_static = Static_scaler\n",
    "flag_save=0\n",
    "Model_Loc='/Desktop/10_150_x_scaler.joblib'\n",
    "Sklean_Save(Model_x,flag_save,Model_Loc)\n",
    "Model_Loc='/Desktop/10_150_y_scaler.joblib'\n",
    "Sklean_Save(Model_y,flag_save,Model_Loc)\n",
    "Model_Loc='/Desktop/10_150_static_scaler.joblib'\n",
    "Sklean_Save(Model_static,flag_save,Model_Loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Architecture and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained=Model_Trained\n",
    "Weight_Loc='/Desktop/10/10_150_12_16.h5'\n",
    "Layout_Loc='/Desktop/10/10_150_12_16.json'\n",
    "Save_Model_Weight(model_trained,Weight_Loc,Layout_Loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result using the best Saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Best_Model_Forecast(Model,PATH_layout,PATH_weight,X_test,y_test,Blind_Static,X_scaler,y_scaler,static_scaler,num_step,feat,thresh,outlier_flag,Method,Static_dim,Pred_flag):\n",
    "        if(PATH_layout ==0 and PATH_weight==0):\n",
    "            X_test= X_scaler.transform(X_test)\n",
    "            y_new_test= y_scaler.transform(y_test.reshape(-1,1))\n",
    "            X_test=LSTM_Reshaping(X_test,num_step,feat)\n",
    "            X_static=static_scaler.transform(Blind_Static)\n",
    "            y_true=y_test.reshape(-1,1)\n",
    "            Pred=LSTM_prediction(X_test,X_static,Model,num_step,feat,y_scaler,Method,Static_dim,Pred_flag)\n",
    "            New_True,New_Pred,outlier=Zero_Removal(y_true,Pred,thresh,outlier_flag)\n",
    "            MAPE = mean_absolute_percentage_error(New_True,np.array(New_Pred))\n",
    "        else:\n",
    "            with open(PATH_layout, 'r') as f:\n",
    "                Model = model_from_json(f.read())\n",
    "\n",
    "            Model.load_weights(PATH_weight)\n",
    "            Model.compile(optimizer='adagrad', loss='mse')\n",
    "            X_test= X_scaler.transform(X_test)\n",
    "            y_new_test= y_scaler.transform(y_test.reshape(-1,1))\n",
    "            X_test=LSTM_Reshaping(X_test,num_step,feat)\n",
    "            y_true=y_test.reshape(-1,1)\n",
    "            X_static=static_scaler.transform(Blind_Static)\n",
    "            Pred=LSTM_prediction(X_test,X_static,Model,num_step,feat,y_scaler,Method,Static_dim,Pred_flag)\n",
    "            New_True,New_Pred,outlier=Zero_Removal(y_true,Pred,thresh,outlier_flag)\n",
    "            MAPE = mean_absolute_percentage_error(New_True,np.array(New_Pred))\n",
    "            \n",
    "        return [MAPE,Pred]   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
